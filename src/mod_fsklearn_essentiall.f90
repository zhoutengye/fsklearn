!---------------------------------------------------------
! mod_fsklearn_essential
!---------------------------------------------------------
!
!   - In this module, some essential parts for the fsklearn
!     are defined. They include three types of essential
!     functions, some files and some derived type variables.
!  - They will be used in mod_fsklearn to build the full
!     machine learning module.
!
!--------------------------------------------------------
! Interfaces:
!--------------------------------------------------------
!
!   - Include the following components:
!   - 1) Read_XX:
!     + Read_Neural_Network
!     + Read_Decision_Tree
!     + Read_Random_Forest
!   - 2) Predict_XX:
!     + Predict_Neural_Network
!     + Predict_Decision_Tree
!     + Predict_Random_Forest
!   - 3) Activation_XX:
!     + Activation_logistic
!     + Activation_tanh
!     + Activation_ReLU
!     + Activation_identity
!     + Activation_softmax
!  - For more details, see the corresponding region for
!    each function.
!
!--------------------------------------------------------
! Files:
!--------------------------------------------------------
!
! - Files associate with Fsklearn. If not set, the path
!   of the training data and the training coefficients
!   will be "fsklearn_data" and "fsklearn_coef" respectively
!
! - By dedault, only the "dsklearn_data", "fsklearn_coef"
!   needed to be set in fsklearn_initialization.
!
! - "coef_files_path":
!   + the path for fsklearn related coefficients
!
! - "set_ML_file":
!   + File name for setting coefficients
!   + Located in the path "coef_files_path"
!   + need to assign input vector length, output vector
!     length, training coefficients and some other
!     custom coefficients
!
! - "f2py_training_coef":
!   + json file that passes the coefficient to Python
!   + Located in the path "coef_files_path"
!
! - "nn_coef.dat":
!   + Neural network coefficients from Python code
!   + Located in the path "coef_files_path"
!
! - "dt_coefficients.dat":
!   + Decision tree coefficients from Python code
!   + Located in the path "coef_files_path"
!
! - "rf_coef.dat":
!   + Random forest coefficients from Python code
!   + Located in the path "coef_files_path"
!
! - "training_data_path":
!   + the path for training data
!
! - "training_py":
!   + python code generated by Fortran code
!
! - "training_data_path":
!   + the path for training data
! -"training_input_name":
!   + File name for input data
!   + Located in the path "training_data_path"
!   + The length of the data in each row is n_inputs
!   + If mpi with N processot is defined, there will be
!     N input files, Then all files will be gathered
!     automatically by the Python code.
!
! - "training_output_name":
!   + Located in the path "training_data_path"
!   + file name for put put data
!   + The length of the data in each row is n_outputs
!     N input files, Then all files will be gathered
!     automatically by the Python code.
!
!--------------------------------------------------------
! Module variables:
!--------------------------------------------------------
!
! PS:
!    - Precision for float number.
!    - PS = 4: single precision
!    - PS = 8: double precision
!
! Fsklearn_Define:
!    - Derived type that contains the machine learning
!       variables. It can do training and prediction
!       with a uniform way.
!    - %n_inputs:
!       + length of the input vector
!       + It is necessary to provide the length of the
!         input vector during initialization.
!    - %n_outputs:
!       + length of the output vector
!       + It is necessary to provide the length of the
!         output vector during initialization.
!    - %Inputs:
!       + Input vector, not necessary
!    - %Outputs:
!       + Output vector, not necessary
!    - %Para_Read:
!       + Read the coefficients and coefficients during
!         initialization for PREDICTION.
!    - %Predict:
!       + Predict erocedure
!    - %Gen_Training:
!       + Procedure for generate training data, can be
!         point to other subroutines.
!    - %Predict:
!       + Procedure for initialization, can be point to
!         other subroutines.
!
! Ragged_Vector:
!    - Declaration of ragged vector and ragged matrix.
!    - In this case, designed for neural networks.
!    - Ragged vectors for 2-D array consists of vectors with
!      different length
!    - Ragged vectors for 3-D array consists of matrice with
!      different sized
!
! NN_activation:
!    - Procedure used to choose activation function
!      at the beginning.
!
! Neural_Network:
!    - Derived Type containing all Neural network variables.
!    - %input_len:
!       + length of the input vector
!    - %output_len:
!       + length of the output vector
!    - %layers:
!       + layer of neural network
!    - %layer_size:
!       + 1-D vector with $(%layers) length, each element
!         each element represents the represents the number
!         of the neural in the corresponding layer.
!    - %Activations:
!       + Activation functions for each layer
!       + Ragged vector with $(%layers)-1 vectors, the length
!         of each vector Activations(i)%Vec is the same as
!         $(layer_size(i))
!    - %Intercepts:
!       + bias variable for each layer
!       + Ragged vector with $(%layers)-1 vectors, the length
!         of each vector Intercepts(i)%Vec is the same as
!         $(layer_size(i))
!    - %Coefs:
!       + coefficient for each neural
!       + Ragged matrix with $(%layers)-1 matrices
!       + The size Coefs(i)%Mat is
!         [$(layer_size(i+1)),$(layer_size(i))]
!    - %Activation type:
!       + Activation function. Contains 5 types:
!        'sigmoid', 'tanh', 'ReLU', 'ideneity', 'softmax'
!    - %Out_activation type:
!       + Activation function for output. Contains 5 types:
!        'sigmoid', 'tanh', 'ReLU', 'ideneity', 'softmax'
!    - %Activation:
!       + Uniform procedure calling activation function
!    - %Out_activation:
!       + Uniform procedure calling out activation function
!    - %Para_Read:
!       + Uniform procedure reading Neural Network Coefficients
!    - %Predict:
!       + Uniform procedure predicting Neural Network results
!
! Nodes:
!    - Derived type for binary tree node class
!    - Used in decision tree and random forest method
!    - %children_left:
!       + Node number of the left child
!    - %children_right:
!       + Node number of the Right child
!    - %feature:
!       + Choice of the input feature for logistic operation
!    - %threshold:
!       + Threshold for logistic operation
!    - %Value:
!       + Value of the predicted result for the corresponding
!         node. with the length of $(n_outputs)
!
! Decision_Tree:
!    - Derived Type containing all Decision Tree variables.
!    - %node_count
!       + amount of the nodes
!    - %n_inputs
!       + length of the input variables
!    - %n_outputs
!       + length of the output variables
!    - %max_depth
!       + maximum depth of the tree
!    - %Node
!       + nodes in decision trees, Nodes type
!
! Random_Forest:
!    - Derived Type containing all Random_Forest variables.
!    - %tree_count
!       + amount of the decision treees
!    - %n_inputs
!       + length of the input variables
!    - %n_outputs
!       + length of the output variables
!    - %Trees
!       + trees in random forest, Decision_Tree type
!
! by Zhouteng Ye
! Last update: 04/17/2019
!
!---------------------------------------------------------

Module Mod_Fsklearn_Essential

  Private

  Public :: Write_Line
  Public :: Fsklearn_IO
  Public :: Neural_Network
  Public :: Decision_Tree
  Public :: Random_Forest

  ! Subject to the choice of precision.
  ! Single precision by default
# if defined(DOUBLE_PRECISION)
  Integer, Parameter :: PS = 8
# else
  Integer, Parameter :: PS = 4
# endif

  Type String
    Character(100) :: str
  End type String

  Type :: Fsklearn_IO
    Character(20) :: training_type
    Character(1000) :: Training_script
    Integer :: n_inputs
    Integer :: n_outputs
    Logical :: train_after_run
    Real(PS), Allocatable :: Inputs(:)
    Real(PS), Allocatable :: Outputs(:)
    Integer :: num_para
    Type(String) , Allocatable :: key(:)
    Type(String) , Allocatable :: value(:)
    Character(100) :: Coef_files_path      = ''
    Character(100) :: Coef_File_Name       = ''
    Character(100) :: set_ML_file          = 'fsklearn_coef.namelist'
    Character(100) :: training_py          = 'training.py'
    Character(100) :: training_data_path   = ''
    Character(100) :: training_input_name  = 'training_input'
    Character(100) :: training_output_name = 'training_output'
  Contains
    Procedure :: Common_Initialization
    Procedure , pass(self) :: Read_Coef => Common_Read_Coef
    Procedure :: Gen_PY => Generate_Training_Python
    Procedure :: Read_Training_Param => Common_Read_and_Update_Param
    Procedure :: Py_Import
    Procedure :: PY_main
    Procedure :: PY_sk2f
  End Type Fsklearn_IO

  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Neural Networks variables↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓

  Type Ragged_Vector
    Real(PS), Allocatable :: Vec(:)
  End Type Ragged_vector

  Type Ragged_Matrix
    Real(PS), Allocatable :: Mat(:,:)
  End Type Ragged_Matrix

  Type :: NN_activation
    Procedure(Sub_Interface), Pointer, NoPass :: activate => NULL()
  End Type NN_activation

  ! interface for choose activation type
  Interface
    Function Sub_Interface(n, X)
      Import :: PS
      Integer,  Intent(in) :: n
      Real(PS), Intent(in), Dimension(n) :: X
      Real(PS), Dimension(n) :: Sub_Interface
    End Function Sub_Interface
  End Interface

  ! neural network coefficients
  Type, Extends(Fsklearn_IO) :: Neural_Network
    Integer :: layers
    Integer, Allocatable :: Layer_Size(:)
    Type(Ragged_Vector), Allocatable :: Activations(:)
    Type(Ragged_Vector), Allocatable :: Intercepts(:)
    Type(Ragged_Matrix), Allocatable :: Coefs(:)
    Character(10) :: Activation_type
    Character(10) :: out_Activation_type
    Type(NN_Activation) :: Activation
    Type(NN_Activation) :: Out_Activation
    Contains
      Procedure :: Read_Coef => NN_Read_Coef
      Procedure :: Predict_One => NN_Predict_One
      ! Procedure :: Predict_Vec => NN_Predict_Vec
      ! Procedure :: Predict_Mat => NN_Predict_Mat
      Procedure :: Read_Training_Param => NN_Read_and_Update_Param
  End Type Neural_Network
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Neural Network Variables↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Decision Tree Variables↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Type Nodes
    Integer  :: children_left
    Integer  :: children_right
    Integer  :: feature
    Real(PS) :: threshold
    Real(PS), Allocatable :: Values(:)
    ! Contains
  End Type Nodes

  type Trees
    Integer :: node_count
    Integer :: max_depth
    Type(Nodes), Allocatable :: Node(:)
  End type Trees

  Type, Extends(Fsklearn_IO) :: Decision_Tree
    Type(Trees) :: Tree
  Contains
    Procedure :: Read_Coef => DT_Read_Coef
    Procedure :: Predict_One => DT_Predict_One
    ! Procedure :: Predict_Vec => DT_Predict_Vec
    ! Procedure :: Predict_Mat => DT_Predict_Mat
    Procedure :: Read_Training_Param => DT_Read_and_Update_Param
  End Type Decision_Tree
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Decision Tree Variables↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Random Forest Variables↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  Type, Extends(Fsklearn_IO) :: Random_Forest
    Integer :: tree_count
    Type(Trees), Allocatable :: Tree(:)
  Contains
    Procedure :: Read_Coef => RF_Read_Coef
    Procedure :: Predict_One => RF_Predict_One
    ! Procedure :: Predict_Vec => RF_Predict_Vec
    ! Procedure :: Predict_Mat => RF_Predict_Mat
    Procedure :: Read_Training_Param => RF_Read_and_Update_Param
  End Type Random_Forest
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Random Forest Variables↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑

  Interface Write_Line
    Module Procedure :: Write_Line_Real
    Module Procedure :: Write_Line_Integer
  end Interface Write_Line


Contains

  Subroutine Common_Initialization(self)

# if defined(PARALLEL)
    Use mpi
# endif
    Implicit None

    Class(Fsklearn_IO) :: self
    Character(20)  :: training_type
    Integer        :: n_inputs
    Integer        :: n_outputs
    Character(100) :: tmp_name
    Character(100) :: str_id
    Logical        :: train_after_run
# if defined(PARALLEL)
    Integer :: myid, ier, ierr, n_proc
# endif

    Namelist /sizes/ n_inputs, n_outputs
    Namelist /train_type/ training_type, train_after_run

    select type (self)
    type is (Fsklearn_IO)
      ! no further initialization required
    class is (Neural_Network)
      self%Training_Type = 'Neural_Network'
      self%num_para = 21
    class is (Decision_Tree)
      self%Training_Type = 'Decision_Tree'
      self%num_para = 12
    class is (Random_Forest)
      self%Training_Type = 'Random_Forest'
      self%num_para = 16
    end select

    Allocate(self%key(self%num_para))
    Allocate(self%value(self%num_para))

    self%training_data_path = 'build/training/'
    self%coef_files_path    = 'build/fsklearn_files/'

    ! set path
    Call Execute_Command_Line('mkdir -p '//self%training_data_path)
    Call Execute_Command_Line('mkdir -p '//self%coef_files_path)

# if defined (PARALLEL)
! mpi version
    ! Read from namelist file
    Call MPI_COMM_RANK(MPI_COMM_WORLD, myid, ier)
    Call MPI_COMM_SIZE(MPI_COMM_WORLD, n_proc, ier)
    If (myid .eq. 0) Then
      tmp_name = Trim(Adjustl(self%coef_files_path))// &
          Trim(Adjustl(self%set_ML_file))
      Open(79, file = tmp_name) ! TR_1 &PR_1

      Read(79, nml = sizes)
      Read(79, nml = train_type)

    End If

    Call MPI_BCAST(n_inputs, 1, MPI_INT, 0, &
        MPI_COMM_WORLD, ier)
    Call MPI_BCAST(n_outputs, 1, MPI_INT, 0, &
        MPI_COMM_WORLD, ier)
    Call MPI_BCAST(training_type, 20, MPI_CHARACTER, 0, &
        MPI_COMM_WORLD, ier)

    self%train_after_run = train_after_run
    self%training_type   = training_type
    self%n_inputs        = n_inputs
    self%n_outputs       = n_outputs

    Write(str_id,"(I10)") myid
    TMP_NAME = TRIM(adjustl(self%training_data_path))// &
        TRIM(adjustl(self%training_input_name))// &
        Trim(adjustl(str_id))// &
        '.dat'
    Open(2000+myid,file=tmp_name,status='unknown')
    TMP_NAME = TRIM(adjustl(self%training_data_path))// &
        TRIM(adjustl(self%training_output_name))// &
        Trim(adjustl(str_id))// &
        '.dat'
    Open(3000+myid,file=tmp_name,status='unknown')

# else
! sequential version
    ! Read from namelist file
    tmp_name = Trim(Adjustl(self%coef_files_path))// &
        Trim(Adjustl(self%set_ML_file))
    Open(79, file = tmp_name)

    Read(79, nml = sizes)
    Read(79, nml = train_type)

    self%training_type  = training_type
    self%n_inputs       = n_inputs
    self%n_outputs       = n_outputs
    self%train_after_run = train_after_run

    tmp_name = Trim(Adjustl(self%training_data_path))// &
        Trim(Adjustl(self%training_input_name))// &
        '.dat'
    Open(2000, file=tmp_name,status='unknown')
    tmp_name = Trim(Adjustl(self%training_data_path))// &
        Trim(Adjustl(self%training_output_name))// &
        '.dat'
    Open(3000, file=tmp_name,status='unknown')

# endif

# if defined (FSKLEARN_TRAINING)
# if defined (PARALLEL)
    If (myid .eq. 0) Then
      tmp_name = Trim(adjustl(self%coef_files_path))// &
          Trim(adjustl(self%training_py))
      Open(77, file = tmp_name)
      Call self%Read_Training_Param(79)
      Call self%Gen_PY(77)
    End If
# else
    tmp_name = Trim(adjustl(self%coef_files_path))// &
        Trim(adjustl(self%training_py))
    Open(77, file = tmp_name)
    Call self%Read_Training_Param(79)
    Call self%Gen_PY(77)
# endif
# endif

# if defined (FSKLEARN_PREDICTION)
    call self%Read_Coef
# endif

  End Subroutine Common_Initialization

  Subroutine Common_Read_Coef(self)
    Implicit None
    Class(Fsklearn_IO) :: self
    print *, 'No training type is assigned, no coefficient will be loaded'
  end Subroutine Common_Read_Coef

  Subroutine NN_Read_Coef(self)
# if defined (PARALLEL)
    Use mpi
# endif
    Implicit None

    Class(Neural_Network) :: self
    Integer :: i,j
    Integer :: error
    Character(100) :: string

    Character :: activation
    Character :: out_activation
    character(100) :: tmp, tmp1
# if defined (PARALLEL)
    Integer :: n_proc
    Integer :: myid
    Character(10) :: string_myid
    Integer :: ier
# endif

    tmp = Trim(adjustl(self%coef_files_path))// &
        Trim(adjustl(self%Coef_File_Name))

# if defined (PARALLEL)
! Parallel version
    ! ugly mpi implementation here
    Call MPI_COMM_RANK(MPI_COMM_WORLD, myid, ier)
    Call MPI_COMM_SIZE(MPI_COMM_WORLD, n_proc, ier)
    Write(string_myid,"(I10)") myid
    tmp1 = Trim(Adjustl(tmp))// &
        Trim(Adjustl(string_myid))
    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('cp '//tmp//' '//tmp1)
      End If
    End Do

    open(4000+myid, file=tmp,status='unknown')

    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) self%layers
    Allocate(self%layer_size(self%layers))

    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) self%layer_size
    self%n_inputs = self%layer_size(1)
    self%n_outputs = self%layer_size(self%layers)

    Allocate(self%activations(self%layers))
    Do i = 1,self%layers
      Allocate(self%activations(i)%vec(self%layer_size(i)))
    End do

    Read(4000+myid, *,iostat=error) string
    Allocate(self%intercepts(self%layers-1))
    Do i = 1,self%layers-1
      Allocate(self%intercepts(i)%vec(self%layer_size(i+1)))
      Read(4000+myid, *) self%intercepts(i)%vec
    End do

    Read(4000+myid, *,iostat=error) string
    Allocate(self%coefs(self%layers-1))
    Do i = 1,self%layers-1
      Allocate(self%coefs(i)%mat(self%layer_size(i+1),self%layer_size(i)))
      Read(4000+myid, *,iostat=error) string
      Do j = 1,self%layer_size(i+1)
        Read(4000+myid, *) self%coefs(i)%mat(j,:)
      End do
    End do

    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) self%Activation_type
    Read(4000+myid, *,iostat=error) string
    Read(4000+myid, *) self%out_Activation_type

    Close(4000+myid)

    if (Trim(selfk%Activation_type).eq.'logistic') Then
      self%activation%activate => Activation_logistic
    else if (Trim(self%Activation_type).eq.'tanh') Then
      self%activation%activate => Activation_tanh
    else if (Trim(self%Activation_type).eq.'softmax') Then
      self%activation%activate => Activation_softmax
    else if (Trim(self%Activation_type).eq.'relu') Then
      self%activation%activate => Activation_ReLU
    else if (Trim(self%Activation_type).eq.'identity') Then
      self%activation%activate => Activation_identity
    else
      write(*,*) 'invalid activation type'
      Stop
    End if

    If (Trim(self%out_Activation_type).eq.'logistic') Then
      self%out_activation%activate => Activation_logistic
    Else If (Trim(self%out_Activation_type).eq.'tanh') Then
      self%out_activation%activate => Activation_tanh
    Else If (Trim(self%out_Activation_type).eq.'softmax') Then
      self%out_activation%activate => Activation_softmax
    Else If (Trim(self%out_Activation_type).eq.'relu') Then
      self%out_activation%activate => Activation_ReLU
    Else If (Trim(self%out_Activation_type).eq.'identity') Then
      self%out_activation%activate => Activation_identity
    Else
      Write(*,*) 'invalid output activation type'
      Stop
    End if
# else
! sequential code

    open(81,file=tmp,status='unknown')

    Read(81,*,iostat=error) string
    Read(81,*) self%layers
    Allocate(self%layer_size(self%layers))

    Read(81,*,iostat=error) string
    Read(81,*) self%layer_size
    self%n_inputs = self%layer_size(1)
    self%n_outputs = self%layer_size(self%layers)


    Allocate(self%activations(self%layers))
    Do i = 1,self%layers
      Allocate(self%activations(i)%vec(self%layer_size(i)))
    End do

    Read(81,*,iostat=error) string
    Allocate(self%intercepts(self%layers-1))
    Do i = 1,self%layers-1
      Allocate(self%intercepts(i)%vec(self%layer_size(i+1)))
      Read(81,*) self%intercepts(i)%vec
    End do


    Read(81,*,iostat=error) string
    Allocate(self%coefs(self%layers-1))
    Do i = 1,self%layers-1
      Allocate(self%coefs(i)%mat(self%layer_size(i+1),self%layer_size(i)))
      Read(81,*,iostat=error) string
      Do j = 1,self%layer_size(i+1)
        Read(81,*) self%coefs(i)%mat(j,:)
      End do
    End do

    Read(81,*,iostat=error) string
    Read(81,*) self%Activation_type
    Read(81,*,iostat=error) string
    Read(81,*) self%out_Activation_type

    Close(81)

    If (Trim(self%Activation_type).eq.'logistic') Then
      self%activation%Activate => Activation_logistic
    Else If (Trim(self%Activation_type).eq.'tanh') Then
      self%activation%Activate => Activation_tanh
    Else If (Trim(self%Activation_type).eq.'softmax') Then
      self%activation%Activate => Activation_softmax
    Else If (Trim(self%Activation_type).eq.'relu') Then
      self%activation%Activate => Activation_ReLU
    Else If (Trim(self%Activation_type).eq.'identity') Then
      self%activation%Activate => Activation_identity
    Else
      Write(*,*) 'invalid activation type'
    End If

    If (Trim(self%out_Activation_type).eq.'logistic') Then
      self%out_activation%Activate => Activation_logistic
    Else If (Trim(self%out_Activation_type).eq.'tanh') Then
      self%out_activation%Activate => Activation_tanh
    Else If (Trim(self%out_Activation_type).eq.'softmax') Then
      self%out_activation%Activate => Activation_softmax
    Else If (Trim(self%out_Activation_type).eq.'relu') Then
      self%out_activation%Activate => Activation_ReLU
    Else If (Trim(self%out_Activation_type).eq.'identity') Then
      self%out_activation%Activate => Activation_identity
    Else
      Write(*,*) 'invalid output activation type'
    End If

# endif

  End Subroutine NN_Read_Coef

  Subroutine DT_Read_Coef(self)
# if defined (PARALLEL)
    Use mpi
# endif
    Implicit None
    Class(Decision_Tree) :: self
    Integer :: i,j
    Integer :: error
    Character(20) :: string
    Character(100) :: tmp, tmp1
# if defined (PARALLEL)
    Integer :: n_proc
    Integer :: myid
    Character(10) :: string_myid
    Integer :: ier
# endif

    tmp = Trim(Adjustl(self%coef_files_path))// &
        Trim(Adjustl(self%Coef_file_Name))

# if defined (PARALLEL)
! Parallel version
    ! ugly mpi implementation here
    Call MPI_COMM_RANK(MPI_COMM_WORLD, myid, ier)
    Call MPI_COMM_SIZE(MPI_COMM_WORLD, n_proc, ier)
    Write(string_myid,"(I10)") myid
    tmp1 = Trim(Adjustl(tmp))// &
        Trim(Adjustl(string_myid))
    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('cp '//tmp//' '//tmp1)
      End If
    End Do

    Open(4000+myid, file=tmp, status='unknown')

    ! tree related coefficients
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) self%Tree%node_count
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) self%n_inputs
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) self%n_outputs
    Read(4000+myid,*,iostat=error) string
    Read(4000+myid,*) self%Tree%max_depth

    Allocate(self%Tree%node(self%tree%node_count))

    ! binary tree
    Do i = 1,self%tree%node_count
      Allocate(self%Tree%node(i)%values(self%n_outputs))
      Read(4000+myid,*,iostat=error) string
      Read(4000+myid,*) self%Tree%node(i)%children_left
      Read(4000+myid,*) self%Tree%node(i)%children_right
      Read(4000+myid,*) self%Tree%node(i)%feature
      Read(4000+myid,*) self%Tree%node(i)%threshold
      Read(4000+myid,*) self%Tree%node(i)%values
    End do

    Close(4000+myid)

    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('rm '//tmp1)
      End If
    End Do

# else
! sequential version
    Open(82, file=tmp, status='unknown')

    Read(82,*,iostat=error) string
    Read(82,*) self%Tree%node_count
    Read(82,*,iostat=error) string
    Read(82,*) self%n_inputs
    Read(82,*,iostat=error) string
    Read(82,*) self%n_outputs
    Read(82,*,iostat=error) string
    Read(82,*) self%Tree%max_depth

    Allocate(self%Tree%node(self%Tree%node_count))

    Do i = 1,self%Tree%node_count
      Allocate(self%Tree%node(i)%values(self%n_outputs))
      Read(82,*,iostat=error) string
      Read(82,*) self%Tree%node(i)%children_left
      Read(82,*) self%Tree%node(i)%children_right
      Read(82,*) self%Tree%node(i)%feature
      Read(82,*) self%Tree%node(i)%threshold
      Read(82,*) self%Tree%node(i)%values
    End do

    Close(82)
# endif

  End Subroutine DT_Read_Coef

  Subroutine RF_Read_Coef(self)
# if defined(PARALLEL)
    Use mpi
# endif
    Implicit None
    Class(Random_Forest) :: self
    Integer :: i,j
    Integer :: error
    Character(100) :: string
    character(100) :: tmp, tmp1
# if defined(PARALELL)
    Integer :: n_proc
    Integer :: myid
    Character(10) :: string_myid
    Integer :: ier
# endif

    tmp = Trim(Adjustl(self%coef_files_path))// &
        Trim(Adjustl(self%Coef_file_Name)) 

# if defined (PARALLEL)
! Parallel version
    ! ugly mpi implementation here
    Call MPI_COMM_RANK(MPI_COMM_WORLD, myid, ier)
    Call MPI_COMM_SIZE(MPI_COMM_WORLD, n_proc, ier)
    Write(string_myid,"(I10)") myid
    tmp1 = Trim(Adjustl(tmp))// &
        Trim(Adjustl(string_myid))
    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('cp '//tmp//' '//tmp1)
      End If
    End Do

    Open(4000+myid, file=tmp, status='unknown')
    Read(4000+myid, *, iostat=error) string
    Read(4000+myid, *) self%tree_count

    Allocate(self%Tree(self%tree_count))

    ! Read from each decision trees
    Do j = 1, self%tree_count
      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) self%Tree(j)%node_count

      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) self%n_inputs

      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) self%n_outputs

      Read(4000+myid,*, iostat=error) string
      Read(4000+myid,*) self%Tree(j)%max_depth

      Allocate(self%Tree(j)%node(self%Tree(j)%node_count))

      Do i = 1, self%Tree(j)%node_count
        Allocate(self%Tree(j)%node(i)%values(self%n_outputs))
        Read(4000+myid, *,iostat=error) string
        Read(4000+myid, *) self%Tree(j)%node(i)%children_left
        Read(4000+myid, *) self%Tree(j)%node(i)%children_right
        Read(4000+myid, *) self%Tree(j)%node(i)%feature
        Read(4000+myid, *) self%Tree(j)%node(i)%threshold
        Read(4000+myid, *) self%Tree(j)%node(i)%values
      End Do
    End Do

    Close(4000+myid)

    Do i = 0, n_proc-1
      If (myid .eq. i) Then
        Call Execute_Command_Line('rm '//tmp1)
      End If
    End Do

# else
!sequential version
    open(83,file=tmp,status='unknown')
    Read(83, *,iostat=error) string
    Read(83, *) self%tree_count

    Allocate(self%tree(self%tree_count))

    Do j = 1, self%Tree_count
      Read(83,*,iostat=error) string
      Read(83,*) self%Tree(j)%node_count
      Read(83,*,iostat=error) string
      Read(83,*) self%n_inputs
      Read(83,*,iostat=error) string
      Read(83,*) self%n_outputs
      Read(83,*,iostat=error) string
      Read(83,*) self%tree(j)%max_depth

      Allocate(self%Tree(j)%node(self%tree(j)%node_count))

      Do i = 1,self%Tree(j)%node_count
        Allocate(self%Tree(j)%node(i)%values(self%n_outputs))
        Read(83,*,iostat=error) string
        Read(83,*) self%Tree(j)%node(i)%children_left
        Read(83,*) self%Tree(j)%node(i)%children_right
        Read(83,*) self%Tree(j)%node(i)%feature
        Read(83,*) self%Tree(j)%node(i)%threshold
        Read(83,*) self%Tree(j)%node(i)%values
      End do
    End do

    Close(83)
# endif

  End Subroutine RF_Read_Coef

  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Predict Subroutines↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  function NN_Predict_One(self, input)
    Implicit None
    Class(Neural_Network) :: self
    Real(PS) :: input(self%n_inputs)
    Real(PS) :: NN_Predict_One(self%n_outputs)
    integer :: i

    self%activations(1)%vec = input

    Do i = 1, self%layers-2
      self%activations(i+1)%vec = matmul(self%coefs(i)%mat, &
          self%activations(i)%vec) + self%intercepts(i)%vec
      self%activations(i+1)%vec = self%activation%activate( &
          self%layer_size(i+1), self%activations(i+1)%vec)
    End do
    self%activations(Self%layers)%vec = &
        matmul(Self%coefs(Self%layers-1)%mat, &
        Self%activations(Self%layers-1)%vec) + &
        Self%intercepts(Self%layers-1)%vec
    Self%activations(Self%layers)%vec = &
        Self%out_activation%activate(Self%n_outputs, &
        Self%activations(Self%layers)%vec)

    NN_Predict_One = Self%activations(Self%layers)%vec

  End function NN_Predict_One

  function DT_Predict_One(self,input)
    Implicit None
    Class(Decision_Tree) :: self
    Real(PS) :: input(self%n_inputs)
    Real(PS) :: DT_Predict_One(self%n_outputs)

    integer :: i,n

    n = 1
    Do i = 1, Self%Tree%max_depth
      if (Self%Tree%node(n)%feature .eq. -1) Exit
      if (input(Self%Tree%node(n)%feature) .le. SELF%Tree%node(n)%threshold) Then
        n = Self%Tree%node(n)%children_left
      else
        n = Self%Tree%node(n)%children_right
      End if
    End do

    DT_Predict_One = Self%Tree%node(n)%values

  End function DT_Predict_One

  function RF_Predict_One(self, input)
    Implicit None
    Class(Random_Forest) :: self
    Real(PS) :: input(self%n_inputs)
    Real(PS) :: RF_Predict_One(self%n_outputs)

    integer :: i, j, n

    RF_Predict_One = 0.0_PS
    Do j = 1, Self%tree_count
      n=1
      Do i = 1, Self%tree(j)%max_depth
        if (Self%tree(j)%node(n)%feature .eq. -1) Exit
        if (input(Self%tree(j)%node(n)%feature) .le. Self%tree(j)%node(n)%threshold) Then
          n = Self%tree(j)%node(n)%children_left
        else
          n = Self%tree(j)%node(n)%children_right
        End if
      End do
      RF_Predict_One = RF_Predict_One + Self%tree(j)%node(n)%values
    End do

    RF_Predict_One = RF_Predict_One / Self%tree_count

  End function RF_Predict_One
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Prediction Subroutines↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Activation functions↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  !-----------------------------------------------------
  ! In this part, the activation functions are defined.
  !   all the activation functions is called by a
  !   uniform procedure "Activation".
  !
  !-----------------------------------------------------
  ! I/O
  !-----------------------------------------------------
  !
  ! inputs:
  !   X:
  !     input vector, should be 1-D
  !   n:
  !     length of the input vector
  ! output:
  !    function result:
  !     output vector, should be 1-D with n length
  !
  !-----------------------------------------------------
  ! Activation functions
  !-----------------------------------------------------
  !
  ! Logistic function:
  !    f(x) = 1/(1+e^-x)
  !
  ! Tanh function:
  !    f(x) = tanh(x)
  !
  ! ReLU function:
  !    f(x) = max(0,x)
  !
  ! Identity function:
  !    f(x) = x
  !
  ! Softmax function:
  !    f(x)_i = exp(x_i)/sigma(x_i)
  !    Only used for classification.
  !    (Which means useless currently)
  !-----------------------------------------------------
  function Activation_logistic(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_logistic
    Activation_logistic = 1.0 / (1.0+exp(-X))
  End function Activation_logistic

  function Activation_tanh(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_tanh
    Activation_tanh = tanh(X)
  End function Activation_tanh

  function Activation_ReLU(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_ReLU
      Activation_ReLU = max(X,0.d0)
    End function Activation_ReLU

  function Activation_identity(n,X)
    Implicit None
    integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: Activation_identity
    Activation_identity = X
  End function Activation_identity

  function Activation_softmax(n,X)
    Implicit None
    Integer, Intent(in) :: n
    Real(PS), Intent(in), dimension(n) :: X
    Real(PS), dimension(n) :: tmp
    Real(PS), dimension(n) :: Activation_softmax
    tmp = exp(X - maxval(X))/sum(tmp)
    Activation_softmax = 1.0 / (1.0+exp(-X))
  End function Activation_softmax
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Activation function↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  !↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓Write_Line↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  ! Subroutine that writes a line of data to the object file
  Subroutine Write_Line_Integer(file_num, vector, length)
    Implicit None

    Integer :: file_num
    Integer :: length
    Integer :: vector(length)

    Write(file_num,*) vector

  End Subroutine Write_Line_Integer

  Subroutine Write_Line_Real(file_num, vector, length)
    Implicit None

    Integer :: file_num
    Integer :: length
    Real(PS) :: vector(length)

    Write(file_num,'(*(F14.6))') vector

  End Subroutine Write_Line_Real
  !↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑End Write_Line↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑

  !↓↓↓↓↓↓↓↓↓↓ Generate_Training_Python file↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
  !
  !  Very ugly subroutines. Simply write to .py file
  !  line by line. Will come back for further development
  !
  Subroutine Generate_Training_Python(self, file_num)
    Implicit None
    Class(FSKLEARN_IO) :: self
    Integer, Intent(in) :: file_num 


  end Subroutine Generate_Training_Python

  Subroutine PY_main(self, file_num)
#if defined (Parallel)
    Use MPI
#endif
    Implicit None
    Class(Fsklearn_IO) :: self
    Integer, Intent(in) :: file_num
    Integer :: num_proc, ier

    write(file_num,'(A)') "T_data_path = '"//Trim(self%training_data_path) // "'"
    write(file_num,'(A)') "coef_path = '"//Trim(self%coef_files_path)//"'"
    write(file_num,'(A)') "training_type = '"//Trim(self%Training_type)//"'"
    write(file_num,'(A)') "input_name = '"//Trim(self%training_input_name)//"'"
    write(file_num,'(A)') "output_name = '"//Trim(self%training_output_name)//"'"
#if defined (Parallel)
    write(file_num,'(A)') "T_input  = np.loadtxt(T_data_path + input_name+'0.dat')"
    write(file_num,'(A)') "T_output  = np.loadtxt(T_data_path + output_name+'0.dat')"
    Call MPI_Comm_size( MPI_COMM_WORLD, num_proc, ier )
    write(file_num,'(A)') "for i in range("//num2str(num_proc-1)//"):"
    write(file_num,'(A)') "    file_name = T_data_path+ input_name + str(i+1) + '.dat'"
    write(file_num,'(A)') "    file_name1 = T_data_path + output_name + str(i+1) + '.dat'"
    write(file_num,'(A)') "    T_input = np.append(T_input,np.loadtxt(file_name), axis=0)"
    write(file_num,'(A)') "    T_output = np.append(T_output,np.loadtxt(file_name1), axis=0)"
#else
    write(file_num,'(A)') "T_input  = np.loadtxt(T_data_path + input_name+ '.dat')"
    write(file_num,'(A)') "T_output  = np.loadtxt(T_data_path + output_name + '.dat')"
#endif

    write(file_num,'(A)') self%Training_script

    write(file_num,'(A)') "ml.fit(T_input, T_output)"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "ml.type = training_type"
    write(file_num,'(A)') "ml.output_coef_path = coef_path"
    write(file_num,'(A)') "sk2f(ml)"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "print('-------------------------------\n')"
    write(file_num,'(A)') "print('training_completed\n\n')"
    write(file_num,'(A)') "print('training_type:\n')"
    write(file_num,'(A)') "print(training_type+'\n')"
    write(file_num,'(A)') "print('-------------------------------\n')"

  End Subroutine PY_main

  Subroutine Py_import(self, file_num)
    Implicit None
    Class(Fsklearn_IO) :: self
    Integer :: file_num
    write(file_num,'(A)') "import json"
    write(file_num,'(A)') "import numpy as np"
    write(file_num,'(A)') "from sklearn.neural_network import MLPRegressor"
    write(file_num,'(A)') "from sklearn.ensemble import RandomForestRegressor"
    write(file_num,'(A)') "from sklearn import tree"
    write(file_num,'(A)') "from sklearn.multioutput import MultiOutputRegressor"
    write(file_num,'(A)') "import os"
    write(file_num,'(A)') ""

  End Subroutine Py_import

  Subroutine PY_sk2f(self, file_num)
    Implicit None
    Class(Fsklearn_IO) :: self
    Integer :: file_num
    write(file_num,'(A)') "def sk2f(write_coef):"
    write(file_num,'(A)') "    if (write_coef.type == 'Neural_Network'):"
    write(file_num,'(A)') "        nn_sk2f(write_coef)"
    write(file_num,'(A)') "    elif (write_coef.type == 'Random_Forest'):"
    write(file_num,'(A)') "        rf_sk2f(write_coef)"
    write(file_num,'(A)') "    elif (write_coef.type == 'Decision_Tree'):"
    write(file_num,'(A)') "        dt_sk2f(write_coef)"
    write(file_num,'(A)') "    else:"
    write(file_num,'(A)') "        print('wrong type')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "def nn_sk2f(nn):"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    file_name = 'nn_coef.dat'"
    write(file_num,'(A)') "    nn_output  = open(nn.output_coef_path+file_name, 'w')"
    write(file_num,'(A)') "    nn_output.write('! n_layers\n%d\n'%(nn.n_layers_))"
    write(file_num,'(A)') "    nn_output.write('! layer_sizes\n')"
    write(file_num,'(A)') "    nn_output.write('%d \n'%np.shape(nn.coefs_[0])[0])"
    write(file_num,'(A)') "    for i in range(len(nn.hidden_layer_sizes)):"
    write(file_num,'(A)') "        nn_output.write('%d '%nn.hidden_layer_sizes[i])"
    write(file_num,'(A)') "    nn_output.write('%d\n'%nn.n_outputs_)"
    write(file_num,'(A)') "    nn_output.write('! intercepts\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    for i in range(len(nn.intercepts_)):"
    write(file_num,'(A)') "        for j in range(len(nn.intercepts_[i])):"
    write(file_num,'(A)') "            nn_output.write('%f '%nn.intercepts_[i][j])"
    write(file_num,'(A)') "        nn_output.write('\n')"
    write(file_num,'(A)') "    nn_output.write('! coefs\n')"
    write(file_num,'(A)') "    for i in range(len(nn.coefs_)):"
    write(file_num,'(A)') "        nn_output.write('!! layer%d\n'%i)"
    write(file_num,'(A)') "        coef = np.transpose(nn.coefs_[i])"
    write(file_num,'(A)') "        for j in range(len(coef)):"
    write(file_num,'(A)') "            for k in range(len(coef[j])):"
    write(file_num,'(A)') "                nn_output.write('%f '%coef[j][k])"
    write(file_num,'(A)') "            nn_output.write('\n')"
    write(file_num,'(A)') "    nn_output.write('! activations\n')"
    write(file_num,'(A)') "    nn_output.write(nn.activation)"
    write(file_num,'(A)') "    nn_output.write('\n')"
    write(file_num,'(A)') "    nn_output.write('! out_activations\n')"
    write(file_num,'(A)') "    nn_output.write(nn.out_activation_)"
    write(file_num,'(A)') "    nn_output.write('\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "def dt_sk2f(dt):"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    dt_output = open(dt.output_coef_path+'dt_coef.dat','w')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    dt_output.write('! node_count\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.node_count)"
    write(file_num,'(A)') "    dt_output.write('! n_features\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.n_features)"
    write(file_num,'(A)') "    dt_output.write('! n_outputs\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.n_outputs)"
    write(file_num,'(A)') "    dt_output.write('! max_depth\n')"
    write(file_num,'(A)') "    dt_output.write('%d\n'%dt.tree_.max_depth)"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    for i in range(dt.tree_.node_count):"
    write(file_num,'(A)') "        dt_output.write('! node %d\n'%(i+1))"
    write(file_num,'(A)') "        dt_output.write('%d\n'%(dt.tree_.children_left[i]+1))"
    write(file_num,'(A)') "        dt_output.write('%d\n'%(dt.tree_.children_right[i]+1))"
    write(file_num,'(A)') "        dt_output.write('%d\n'%(dt.tree_.feature[i]+1))"
    write(file_num,'(A)') "        dt_output.write('%f\n'%dt.tree_.threshold[i])"
    write(file_num,'(A)') "        for j in range(dt.tree_.n_outputs):"
    write(file_num,'(A)') "            dt_output.write('%f '%dt.tree_.value[i,j])"
    write(file_num,'(A)') "        dt_output.write('\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "def rf_sk2f(rf):"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    rf_output = open(rf.output_coef_path+'rf_coef.dat','w')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    rf_output.write('! tree_count\n')"
    write(file_num,'(A)') "    rf_output.write('%d\n'%len(rf.estimators_))"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    trees = rf.estimators_"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "    for j in range(len(rf.estimators_)):"
    write(file_num,'(A)') "        tree1 = trees[j].tree_"
    write(file_num,'(A)') "        rf_output.write('! node_count\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.node_count)"
    write(file_num,'(A)') "        rf_output.write('! n_features\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.n_features)"
    write(file_num,'(A)') "        rf_output.write('! n_outputs\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.n_outputs)"
    write(file_num,'(A)') "        rf_output.write('! max_depth\n')"
    write(file_num,'(A)') "        rf_output.write('%d\n'%tree1.max_depth)"
    write(file_num,'(A)') ""
    write(file_num,'(A)') "        for i in range(tree1.node_count):"
    write(file_num,'(A)') "            rf_output.write('! node %d\n'%(i+1))"
    write(file_num,'(A)') "            rf_output.write('%d\n'%(tree1.children_left[i]+1))"
    write(file_num,'(A)') "            rf_output.write('%d\n'%(tree1.children_right[i]+1))"
    write(file_num,'(A)') "            rf_output.write('%d\n'%(tree1.feature[i]+1))"
    write(file_num,'(A)') "            rf_output.write('%f\n'%tree1.threshold[i])"
    write(file_num,'(A)') "            for j in range(tree1.n_outputs):"
    write(file_num,'(A)') "                rf_output.write('%f '%tree1.value[i,j])"
    write(file_num,'(A)') "            rf_output.write('\n')"
    write(file_num,'(A)') ""
    write(file_num,'(A)') ""

  End Subroutine PY_SK2F
  !↑↑↑↑↑↑↑↑↑↑↑↑↑End generating python file↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑


  Subroutine Common_Read_and_Update_Param(self, file_num)
    Implicit None
    Class(Fsklearn_IO) :: self
    Integer, Intent(in) :: file_num
    print *, 'No training type is assigned, no coefficient will be loaded'
  end Subroutine Common_Read_and_Update_Param

  Subroutine NN_Read_and_Update_Param(self, file_num)
    Implicit None

    Class(Neural_Network) :: self
    Integer, Intent(In) :: file_num
    Character(50) :: hidden_layer_sizes = "NULL"
    Character(50) :: activation = "NULL"
    Character(50) :: solver = "NULL"
    Character(50) :: alpha = "NULL"
    Character(50) :: batch_size = "NULL"
    Character(50) :: learning_rate = "NULL"
    Character(50) :: learning_rate_init = "NULL"
    Character(50) :: power_t = "NULL"
    Character(50) :: max_iter = "NULL"
    Character(50) :: shuffle = "NULL"
    Character(50) :: random_state = "NULL"
    Character(50) :: tol = "NULL"
    Character(50) :: verbose = "NULL"
    Character(50) :: warm_start = 'NULL'
    Character(50) :: momentum = "NULL"
    Character(50) :: nesterovs_momentum = "NULL"
    Character(50) :: early_stopping = "NULL"
    Character(50) :: validation_fraction = "NULL"
    Character(50) :: beta_1 = "NULL"
    Character(50) :: beta_2 = "NULL"
    Character(50) :: epsilon = "NULL"

    Namelist /NN_Parameter/ hidden_layer_sizes, &
        activation, &
        solver, &
        alpha, &
        batch_size, &
        learning_rate, &
        learning_rate_init, &
        power_t, &
        max_iter, &
        shuffle, &
        random_state, &
        tol, &
        verbose, &
        warm_start, &
        momentum, &
        nesterovs_momentum, &
        early_stopping, &
        validation_fraction, &
        beta_1, &
        beta_2, &
        epsilon

    self%key(1)%str= 'hidden_layer_sizes'
    self%value(1)%str= "(100,)"

    self%key(2)%str = 'activation'
    self%value(2)%str = "'relu'"

    self%key(3)%str = 'solver'
    self%value(3)%str = "'adam'"

    self%key(4)%str = 'alpha'
    self%value(4)%str = "0.0001"

    self%key(5)%str = 'batch_size'
    self%value(5)%str = "'auto'"

    self%key(6)%str = 'learning_rate'
    self%value(6)%str = "'constant'"

    self%key(7)%str = 'learning_rate_init'
    self%value(7)%str = "0.001"

    self%key(8)%str = 'power_t'
    self%value(8)%str = "0.5"

    self%key(9)%str = 'max_iter'
    self%value(9)%str = "200"

    self%key(10)%str = 'shuffle'
    self%value(10)%str = "True"

    self%key(11)%str = 'random_state'
    self%value(11)%str = "None"

    self%key(12)%str = 'tol'
    self%value(12)%str = "0.0001"

    self%key(13)%str = 'verbose'
    self%value(13)%str = "False"

    self%key(14)%str = 'warm_start'
    self%value(14)%str = "False"

    self%key(15)%str = 'momentum'
    self%value(15)%str = "True"

    self%key(16)%str = 'nesterovs_momentum'
    self%value(16)%str = "True"

    self%key(17)%str = 'early_stopping'
    self%value(17)%str = "False"

    self%key(18)%str = "validation_fraction"
    self%value(18)%str = "0.1"

    self%key(19)%str = 'beta_1'
    self%value(19)%str = "0.9"

    self%key(20)%str = 'beta_2'
    self%value(20)%str = "0.999"

    self%key(21)%str = 'epsilon'
    self%value(21)%str = "1e-08"


    Read(file_num, nml = NN_Parameter)

    If (hidden_layer_sizes  .ne. 'NULL') Then
      self%value(1)%str = hidden_layer_sizes 
    End If

    If (activation .ne. 'NULL') Then
      self%value(2)%str = activation
    End If

    If (solver .ne. 'NULL') Then
      self%value(3)%str = solver
    End If

    If (alpha .ne. 'NULL') Then
      self%value(4)%str = alpha
    End If

    If (batch_size .ne. 'NULL') Then
      self%value(5)%str = batch_size
    End If

    If (learning_rate .ne. 'NULL') Then
      self%value(6)%str = learning_rate
    End If

    If (learning_rate_init .ne. 'NULL') Then
      self%value(7)%str = learning_rate_init
    End If

    If (power_t .ne. 'NULL') Then
      self%value(8)%str = power_t
    End If

    If (max_iter .ne. 'NULL') Then
      self%value(9)%str = max_iter
    End If

    If (shuffle .ne. 'NULL') Then
      self%value(10)%str = shuffle
    End If

    If (random_state .ne. 'NULL') Then
      self%value(11)%str = random_state
    End If

    If (tol .ne. 'NULL') Then
      self%value(12)%str = tol
    End If

    If (verbose .ne. 'NULL') Then
      self%value(13)%str = verbose
    End If

    If (warm_start .ne. 'NULL') Then
      self%value(14)%str = warm_start
    End If

    If (momentum .ne. 'NULL') Then
      self%value(15)%str = momentum
    End If

    If (nesterovs_momentum .ne. 'NULL') Then
      self%value(16)%str = nesterovs_momentum
    End If

    If (early_stopping .ne. 'NULL') Then
      self%value(17)%str = early_stopping
    End If

    If (validation_fraction .ne. 'NULL') Then
      self%value(18)%str = validation_fraction
    End If

    If (beta_1 .ne. 'NULL') Then
      self%value(19)%str = beta_1
    End If

    If (beta_2 .ne. 'NULL') Then
      self%value(20)%str = beta_2
    End If

    If (epsilon .ne. 'NULL') Then
      self%value(21)%str = epsilon
    End If

  end Subroutine NN_Read_and_Update_Param

  Subroutine DT_Read_and_Update_Param(self, file_num)
    Implicit None

    Class(Decision_Tree) :: self
    Integer, intent(in) :: file_num
    Character(50) :: criterion ='NULL'
    Character(50) :: splitter ='NULL'
    Character(50) :: max_depth = 'NULL'
    Character(50) :: min_samples_split = 'NULL'
    Character(50) :: min_samples_leaf = 'NULL'
    Character(50) :: min_weight_fraction_leaf = 'NULL'
    Character(50) :: max_features = 'NULL'
    Character(50) :: random_state = 'NULL'
    Character(50) :: max_leaf_nodes ='NULL'
    Character(50) :: min_impurity_decrease = 'NULL'
    Character(50) :: min_impurity_split = 'NULL'
    Character(50) :: presort = 'NULL'

    Namelist /DT_Parameter/ criterion , &
        splitter , &
        max_depth , &
        min_samples_split , &
        min_samples_leaf , &
        min_weight_fraction_leaf , &
        max_features , &
        random_state , &
        max_leaf_nodes , &
        min_impurity_decrease , &
        min_impurity_split , &
        presort 

    self%key(1)%str = "criterion"
    self%value(1)%str = "'mse'"

    self%key(2)%str = "splitter"
    self%value(2)%str = "'best'"

    self%key(3)%str = "max_depth"
    self%value(3)%str = "None"

    self%key(4)%str = "min_samples_split"
    self%value(4)%str = "2 "

    self%key(5)%str = "min_samples_leaf"
    self%value(5)%str = "1"

    self%key(6)%str = "min_weight_fraction_leaf"
    self%value(6)%str = "0.0"

    self%key(7)%str = "max_features"
    self%value(7)%str = "None"

    self%key(8)%str = "random_state"
    self%value(8)%str = "None"

    self%key(9)%str = "max_leaf_nodes"
    self%value(9)%str = "None"

    self%key(10)%str = "min_impurity_decrease"
    self%value(10)%str = "0.0"

    self%key(11)%str = "min_impurity_split"
    self%value(11)%str = "None"

    self%key(12)%str = "presort "
    self%value(12)%str = "False"

    Read(file_num, nml = DT_Parameter)

    If (criterion  .ne. 'NULL') Then
      self%value(1)%str = criterion 
    End If

    If (splitter  .ne. 'NULL') Then
      self%value(2)%str = splitter 
    End If

    If (max_depth  .ne. 'NULL') Then
      self%value(3)%str = max_depth 
    End If

    If (min_samples_split  .ne. 'NULL') Then
      self%value(4)%str = min_samples_split 
    End If

    If (min_samples_leaf  .ne. 'NULL') Then
      self%value(5)%str = min_samples_leaf 
    End If

    If (min_weight_fraction_leaf  .ne. 'NULL') Then
      self%value(6)%str = min_weight_fraction_leaf 
    End If

    If (max_features  .ne. 'NULL') Then
      self%value(7)%str = max_features 
    End If

    If (random_state  .ne. 'NULL') Then
      self%value(8)%str = random_state 
    End If

    If (max_leaf_nodes  .ne. 'NULL') Then
      self%value(9)%str = max_leaf_nodes 
    End If

    If (min_impurity_decrease  .ne. 'NULL') Then
      self%value(10)%str = min_impurity_decrease 
    End If

    If (min_impurity_split  .ne. 'NULL') Then
      self%value(11)%str = min_impurity_split 
    End If

    If (presort  .ne. 'NULL') Then
      self%value(12)%str = presort 
    End If


  end Subroutine DT_Read_and_Update_Param


  Subroutine RF_Read_and_Update_Param(self, file_num)
    Implicit None
    Class(Random_Forest) :: self
    Integer, intent(in) :: file_num

    Character(50) :: n_estimators= 'NULL'
    Character(50) :: criterion= 'NULL'
    Character(50) :: max_depth= 'NULL'
    Character(50) :: min_samples_split= 'NULL'
    Character(50) :: min_samples_leaf= 'NULL'
    Character(50) :: min_weight_fraction_leaf= 'NULL'
    Character(50) :: max_features= 'NULL'
    Character(50) :: max_leaf_nodes= 'NULL'
    Character(50) :: min_impurity_decrease= 'NULL'
    Character(50) :: min_impurity_split= 'NULL'
    Character(50) :: bootstrap= 'NULL'
    Character(50) :: oob_score= 'NULL'
    Character(50) :: n_jobs= 'NULL'
    Character(50) :: random_state= 'NULL'
    Character(50) :: verbose= 'NULL'
    Character(50) :: warm_start= 'NULL'

    Namelist /RF_Parameter/ n_estimators, &
        criterion, &
        max_depth, &
        min_samples_split, &
        min_samples_leaf, &
        min_weight_fraction_leaf, &
        max_features, &
        max_leaf_nodes, &
        min_impurity_decrease, &
        min_impurity_split, &
        bootstrap, &
        oob_score, &
        n_jobs, &
        random_state, &
        verbose, &
        warm_start

    self%key(1)%str = "n_estimators"
    self%value(1)%str = "'warn' "

    self%key(2)%str = "criterion"
    self%value(2)%str = "'mse' "

    self%key(3)%str = "max_depth"
    self%value(3)%str = "None"

    self%key(4)%str = "min_samples_split"
    self%value(4)%str = "2 "

    self%key(5)%str = "min_samples_leaf"
    self%value(5)%str = "1"

    self%key(6)%str = "min_weight_fraction_leaf"
    self%value(6)%str = "0.0 "

    self%key(7)%str = "max_features"
    self%value(7)%str = "'auto' "

    self%key(8)%str = "max_leaf_nodes"
    self%value(8)%str = "None"

    self%key(9)%str = "min_impurity_decrease"
    self%value(9)%str = "0.0 "

    self%key(10)%str = "min_impurity_split"
    self%value(10)%str = "None"

    self%key(11)%str = "bootstrap"
    self%value(11)%str = "True "

    self%key(12)%str = "oob_score"
    self%value(12)%str = "False "

    self%key(13)%str = "n_jobs"
    self%value(13)%str = "None"

    self%key(14)%str = "random_state"
    self%value(14)%str = "None "

    self%key(15)%str = "verbose"
    self%value(15)%str = "0 "

    self%key(16)%str = "warm_start"
    self%value(16)%str = "False"

    If (n_estimators .ne. 'NULL') Then
      self%value(1)%str = n_estimators
    End If

    If (criterion .ne. 'NULL') Then
      self%value(2)%str = criterion
    End If

    If (max_depth .ne. 'NULL') Then
      self%value(3)%str = max_depth
    End If

    If (min_samples_split .ne. 'NULL') Then
      self%value(4)%str = min_samples_split
    End If

    If (min_samples_leaf .ne. 'NULL') Then
      self%value(5)%str = min_samples_leaf
    End If

    If (min_weight_fraction_leaf .ne. 'NULL') Then
      self%value(6)%str = min_weight_fraction_leaf
    End If

    If (max_features .ne. 'NULL') Then
      self%value(7)%str = max_features
    End If

    If (max_leaf_nodes .ne. 'NULL') Then
      self%value(8)%str = max_leaf_nodes
    End If

    If (min_impurity_decrease .ne. 'NULL') Then
      self%value(9)%str = min_impurity_decrease
    End If

    If (min_impurity_split .ne. 'NULL') Then
      self%value(10)%str = min_impurity_split
    End If

    If (bootstrap .ne. 'NULL') Then
      self%value(11)%str = bootstrap
    End If

    If (oob_score .ne. 'NULL') Then
      self%value(12)%str = oob_score
    End If

    If (n_jobs .ne. 'NULL') Then
      self%value(13)%str = n_jobs
    End If

    If (random_state .ne. 'NULL') Then
      self%value(14)%str = random_state
    End If

    If (verbose .ne. 'NULL') Then
      self%value(15)%str = verbose
    End If

    If (warm_start .ne. 'NULL') Then
      self%value(16)%str = warm_start
    End If

  end Subroutine RF_Read_and_Update_Param

  Subroutine Gen_Para_Script(self, file_num)
    Implicit None
    Class(Fsklearn_IO) :: self
    Integer, Intent(in) :: file_num
    Character(1000) :: Temp
    Integer :: i

    Call self%Read_Training_Param(file_num)

    select type (self)
    type is (Fsklearn_IO)
      ! no further initialization required
    class is (Neural_Network)
      Temp = 'ml = MLPRegressor('
    class is (Decision_Tree)
      Temp = 'ml = tree.DecisionTreeRegressor('
    class is (Random_Forest)
      Temp = 'ml = tree.RandomForestRegressor('
    end select

    Do i = 1, self%num_para - 1
      Temp = Trim(Temp)//Trim(self%key(i)%str) // &
          ' = ' // Trim(self%value(i)%str) // ','
    End Do
    i = self%num_para
    self%Training_Script = Trim(Temp)//Trim(self%key(i)%str) // &
        ' = ' // Trim(self%value(i)%str) // ')'


  end Subroutine Gen_Para_Script


End Module Mod_Fsklearn_Essential
